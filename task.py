# -*- coding: utf-8 -*-
"""task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wuwMTDxfOnL9zp83XAJb3cGod5SnBRGL
"""



"""# Abstract
Business problem
- banks faceses heigh loss due to defaulters , we have to predict person will be defaulter or not with probablity of him to be defaulter

ML foormulaion
- this is binary classification task 
- in this task we have predict probablity of person to be defaulter or not
- in this problem it is important to have less false posiitve rate becase it can costs heigh loss to the bank 
- matix for evaluation is auc matrix
- interpritability is important to give beacuse of which features preson should be defaulter one 

first cut approach
- first we will perform basic EDA 
- Handling of missing data
- Handle imbalence dataset
- after data perparation we will add some new features 
- Do required preprocessing over data (numerical ,categorical and text features)
- apply ml model
- predict submission
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %pip install chart-studio
import warnings
warnings.filterwarnings("ignore")

import sqlite3
import pandas as pd
import numpy as np
import nltk
import string
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.metrics import roc_curve, auc
from nltk.stem.porter import PorterStemmer
from sklearn.impute import SimpleImputer

import re
# Tutorial about Python regular expressions: https://pymotw.com/2/re/
import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer

from gensim.models import Word2Vec
from gensim.models import KeyedVectors
import pickle

from tqdm import tqdm
import os

from chart_studio.plotly import plotly
import plotly.offline as offline
import plotly.graph_objs as go
offline.init_notebook_mode()
from collections import Counter

train_data = pd.read_csv('train_indessa.csv')
test_data = pd.read_csv('test_indessa.csv')

train_data.info()

dtypes = train_data.dtypes
categorical = dtypes[dtypes=='object']
print('following are categorical features')
catgs = np.array(categorical.keys())
cat_feature = catgs[catgs != 'desc']
print(cat_feature)

print('desc is text feature')

dtypes = train_data.dtypes
categorical = dtypes[dtypes!='object']
print('following are numeric features')
int_features = np.array(categorical.keys())

print(int_features , int_features.shape[0])

print("Number of data points in train data", train_data.shape)
print('-'*50)
print("The attributes of data :", train_data.columns.values)

y_value_counts = train_data['loan_status'].value_counts()
print("Number of members thar are Defaulters ", y_value_counts[1], ", (", (y_value_counts[1]/(y_value_counts[1]+y_value_counts[0]))*100,"%)")
print("Number of projects thar are non defaulters ", y_value_counts[0], ", (", (y_value_counts[0]/(y_value_counts[1]+y_value_counts[0]))*100,"%)")

fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(aspect="equal"))
recipe = ["Defaulters", "Non Defaulters"]

data = [y_value_counts[1], y_value_counts[0]]

wedges, texts = ax.pie(data, wedgeprops=dict(width=0.5), startangle=-40)

bbox_props = dict(boxstyle="square,pad=0.3", fc="w", ec="k", lw=0.72)
kw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle="-"),
          bbox=bbox_props, zorder=0, va="center")

for i, p in enumerate(wedges):
    ang = (p.theta2 - p.theta1)/2. + p.theta1
    y = np.sin(np.deg2rad(ang))
    x = np.cos(np.deg2rad(ang))
    horizontalalignment = {-1: "right", 1: "left"}[int(np.sign(x))]
    connectionstyle = "angle,angleA=0,angleB={}".format(ang)
    kw["arrowprops"].update({"connectionstyle": connectionstyle})
    ax.annotate(recipe[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),
                 horizontalalignment=horizontalalignment, **kw)

ax.set_title("All defaulter and non defaulters")

plt.show()

"""### we can say that data is imbalence data 


1.   we will chosse to understamplig to deal with an imbalence dataset 
2.   we will choose over sampling to deal with imbalence data because with undersampling we dont want to loose the information
3.   we will goin to do oversampling of minority class upto 2:3 ration


"""

blank_spaces = train_data.eq(' ').sum()
blank_spaces

# we will check nan values in overall dataset
# review the columns with heighest amount of nan values
na_values = train_data.isna().sum()
na_values

#columns with nan value
train_data.isna().any(axis=0).sum().sum()

"""- so we can find there are huge no of nan values in some columns so we will going to exclude those columns whose contains more that 50 % of nan values 
- for features whose are beetwen 0 to 50 % of nan values we will going replace them with median if they are numerical features and if they are categorical then we will going to replace them by most occuring category
"""

# Creating a dictionary whose keys are the column names and values are the percentage of missing values
x = train_data
nan_count = {k:list(x.isna().sum()*100/x.shape[0])[i] for i,k in enumerate(x.columns)}

# Sorting the dictionary in descending order based on the percentage of missing values
nan_count = {k: v for k, v in sorted(nan_count.items(), key=lambda item: item[1],reverse=True)}

# Plotting a graph showing the top 15 features having highest percentage of missing values 
sns.set_style(style="whitegrid")
plt.figure(figsize=(20,10))

# Bar Plot
plot = sns.barplot(x= list(nan_count.keys())[:15],y = list(nan_count.values())[:15],palette="hls")

# Add annotations above each bar signifying their value
for p in plot.patches:
        plot.annotate('{:.1f}%'.format(p.get_height()), (p.get_x()+0.2, p.get_height()+1))

# Make y-axis more interpretable
plot.set_yticklabels(map('{:.1f}%'.format, plot.yaxis.get_majorticklocs())) 
plt.show()

print('columns whose having nan values more than 50% ')
x = nan_count
sorted_x = sorted(x.items(), key=lambda kv: kv[1], reverse=True)
heigh_na_features = {i:j for i,j in sorted_x if j > 50}
print(heigh_na_features)
print('columns whose having nan values less than 50%')
medium_na_features = {i:j for i,j in sorted_x if j <= 50 and j > 0} 
print(medium_na_features)

train_data_preprocessed = train_data
test_data_preprocessed = test_data

#drop the columns having missing values rate > 50%
train_data_preprocessed = train_data_preprocessed.drop(columns=['verification_status_joint', 'desc', 'mths_since_last_record', 'mths_since_last_major_derog', 'mths_since_last_delinq'])

train_data_preprocessed.shape



"""# Handle missing values

- batch batch_enrolled feature contains nan values + blank spaces 
- this is total nearly 35.9 % of total rows 
- as we are not domain expert we cant tell values whcih are missing are missing reandomly or the missing value have its own meaning 
- there are three options in this case
   - remove column 
   - replace spaces and nan with most frequest value 
   - conside  blank space and nan as new categories
- we will choose third option
"""

(blank_spaces['batch_enrolled']+na_values['batch_enrolled'])/train_data.shape[0]

train_data_preprocessed['batch_enrolled'] = train_data_preprocessed['batch_enrolled'].fillna('unknown')

train_data_preprocessed['batch_enrolled'] = train_data_preprocessed['batch_enrolled'].map(lambda x: 'blank' if x == ' ' else x)

batch_enrolled = train_data_preprocessed['batch_enrolled']
print(batch_enrolled[batch_enrolled=='blank'].shape , batch_enrolled[batch_enrolled=='unknown'].shape)

train_data_preprocessed['batch_enrolled']

medium_na_features
categorical_na_features = list(filter(lambda x: x in cat_feature,list(medium_na_features.keys())))
int_na_features = list(filter(lambda x: x in int_features,list(medium_na_features.keys())))

categorical_na_features

int_na_features

def replace_with_most_frequent (feature):
   values_data = train_data_preprocessed[feature].value_counts()
   max_value = values_data[values_data == values_data.max()].index[0]
   return train_data_preprocessed[feature].fillna(max_value[0]) , max_value

def fill_cat_na(categorical_na_features):
  save_feature_max_value = dict({})
  for i in categorical_na_features:
    feature , val =  replace_with_most_frequent(i)
    train_data_preprocessed[i] = feature
    save_feature_max_value[i] = val
  pickle.dump(save_feature_max_value, file = open("save_feature_max_value.pickle", "wb"))
  save_feature_max_value = pickle.load(open("save_feature_max_value.pickle", "rb"))
  return save_feature_max_value

fill_cat_na(categorical_na_features)

save_feature_max_value = pickle.load(open("save_feature_max_value.pickle", "rb"))
save_feature_max_value

def replace_with_median (feature):
    median_value =  train_data_preprocessed[feature].median()
    train_feature = train_data_preprocessed[feature].fillna(median_value)
    return train_feature , median_value

def fill_int_na(int_na_features):
  save_feature_median_value = dict({})
  for i in int_na_features:
    feature , val =  replace_with_median(i)
    train_data_preprocessed[i] = feature
    save_feature_median_value[i] = val
  pickle.dump(save_feature_median_value, file = open("save_feature_median_value.pickle", "wb"))
  save_feature_median_value = pickle.load(open("save_feature_median_value.pickle", "rb"))
  return save_feature_median_value

fill_int_na(int_na_features)

train_data_preprocessed.isna().sum()

"""#Basic EDA"""

dtypes = train_data_preprocessed.dtypes
categorical = dtypes[dtypes=='object']
print('following are categorical features')
catgs = np.array(categorical.keys())
preprocess_cat_feature = catgs[catgs != 'desc']
print(preprocess_cat_feature ,preprocess_cat_feature.shape)

dtypes = train_data_preprocessed.dtypes
categorical = dtypes[dtypes!='object']
print('following are numerical features')
catgs = np.array(categorical.keys())
preprocess_int_feature = catgs[np.logical_and(catgs != 'member_id', catgs != 'loan_status') ]
print(preprocess_int_feature.shape)
preprocess_int_feature

def printCounterPlot(feature , row): 
  # count plot on two categorical variable
  if train_data_preprocessed[feature].value_counts().shape[0] > 30:
    return
  sns.countplot(x = feature, hue = "loan_status", data = train_data_preprocessed)
  plt.show()

train_data_preprocessed.loan_status.value_counts()

### Data Visualization libraries
import seaborn as sns
import matplotlib.pyplot as plt


for idx,cat_col in enumerate(preprocess_cat_feature):
    printCounterPlot(cat_col,idx)
#plt.subplots_adjust(hspace=1)

train_data_preprocessed.application_type.value_counts()

application_type = train_data_preprocessed[train_data_preprocessed.loan_status == 0].application_type
application_type[application_type == 'JOINT'].shape[0]

application_type = train_data_preprocessed[train_data_preprocessed.loan_status == 1].application_type
application_type[application_type == 'JOINT'].shape[0]

train_data_preprocessed.pymnt_plan.value_counts()

pymnt_plan = train_data_preprocessed[train_data_preprocessed.loan_status == 0].pymnt_plan
application_type[application_type == 'y'].shape[0]

train_data_preprocessed = train_data_preprocessed.drop(columns=['application_type', 'pymnt_plan'])

"""- in our dataset if we see mostly all members having individual application type we so if we drop this feature it will not affect result much 
- same is about pymnt_plan feature almost all the members having pymnt_plan category n so this feature can misguide us
- we will drop this two features from categorical features 
"""

def plotPdf(feature):  
  defaulter = train_data_preprocessed[train_data_preprocessed['loan_status']==1][feature].values
  non_defaulter = train_data_preprocessed[train_data_preprocessed['loan_status']==0][feature].values
  plt.figure(figsize=(10,3))
  sns.distplot(defaulter, hist=False, label="Defaulters")
  sns.distplot(non_defaulter, hist=False, label="Non Defaulters")
  plt.title(feature+' pdf')
  plt.xlabel(feature+' range')
  plt.legend()
  plt.show()

for idx,cat_col in enumerate(preprocess_int_feature):
    plotPdf(cat_col)

train_data_preprocessed[train_data_preprocessed.loan_status == 1].collection_recovery_fee.value_counts()

train_data_preprocessed[train_data_preprocessed.loan_status == 1].recoveries.value_counts()

"""- from abouve analysis we can say that 'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate' these are some imp features becase they show quite good seperation of defaulter and no defaulter pdf areas

- collection_recovery_fee and recoveries having zero std deviation for defauters i.e all values for defauter cases for this two features are zero
"""

def text_operations(features):
  for i in features:
    train_data_preprocessed[i] = train_data_preprocessed[i].map(lambda x: x.lower().replace(' ','_'))



text_operations(['term','batch_enrolled','grade','sub_grade','emp_title','emp_length'
,'home_ownership','verification_status','purpose','title'
,'zip_code','addr_state','initial_list_status'
,'last_week_pay'])

plt.figure(figsize=(20,11))
sns.heatmap(train_data_preprocessed.corr(),annot=True)
plt.title("Correlation Matrix")
plt.show()

"""

1.   loan amount , funded_amount and funded_amount_inv have correlation of 1 with each other that can will reduce model performence 
2.   we will drop funded_amount and funded_amount_inv

"""

train_data_preprocessed = train_data_preprocessed.drop(columns=['funded_amnt', 'funded_amnt_inv'])

"""# create new feature
- we dont have experties in bank domain , generally we can see if there is to much of loan with respect to monthly income then person will no able to complete loan 
- new feature = log(loan_amount + intrest_rate * term_of_loan)+0.00001/log(anual_income)+0.00001
- used log to reduce correlaiton and  0.00001 noice prevent us from devide by zero error
- if pdf of this feature will not show significant seperation then we will take this feature  
"""

import math
train_data_preprocessed['paybility'] = train_data_preprocessed.apply(lambda row: (math.log(row.loan_amnt + (float(row.term.split('_')[0]) * row.int_rate)) + 0.00001 )/math.log(row.annual_inc), axis=1)

train_data_preprocessed['paybility']

plotPdf('paybility')

"""- its seperating regions cosiderable we will keep this feature

# Modeling 
- train test split
- oversampling of train dataset
- normalize numerical features as we dont want to loose original distribution of dataser
- one hot encoading of categorical features
- after dataset is ready load into model we will apply (Logistic regression , GBDT , RandomForest)
  - cross validation using random serach
  - plot auc-roc value against epocs
  - draw roc cusrve on both train and test
  - draw confussion metrix in from best threshold
- we will also try small mlp nural network with dence layers
- compaire models and select model having heighest AUC-ROC score
- predict submission data
"""

#train_data_preprocessed.to_pickle("train_data_preprocessed.pkl")
train_data_preprocessed = pd.read_pickle("train_data_preprocessed.pkl")

train_data_preprocessed.shape

y = train_data_preprocessed['loan_status']
X_data = train_data_preprocessed.drop(columns=['loan_status'])

from sklearn.model_selection import train_test_split
X_train_old, X_test, y_train_old, y_test = train_test_split(X_data, y,
                                                    stratify=y, 
                                                    test_size=0.2, random_state=42)

#!pip install -U imbalanced-learn
from imblearn.over_sampling import RandomOverSampler
from collections import Counter

print(Counter(y_train_old))
# define oversampling strategy
oversample = RandomOverSampler(sampling_strategy=0.5,random_state=42)
# fit and apply the transform
X_over, y_over = oversample.fit_resample(X_train_old, y_train_old)
# summarize class distribution
print(Counter(y_over))

X_train , y_train = X_over, y_over

"""# normalizing numerical features"""

from sklearn.preprocessing import Normalizer

numerical_features = ['loan_amnt', 'int_rate',
       'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc',
       'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'total_rec_int',
       'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',
       'collections_12_mths_ex_med', 'acc_now_delinq', 'tot_coll_amt',
       'tot_cur_bal', 'total_rev_hi_lim','paybility']

X_test.shape

X_train.shape

X_train_numerical_features = X_train[numerical_features].values
X_test_numerical_features = X_test[numerical_features].values

X_train[numerical_features]

from sklearn.preprocessing import MinMaxScaler
def normalize_feature(nmumerical_fetures):
  X_train_numerical_features = X_train[nmumerical_fetures].values
  X_test_numerical_features = X_test[nmumerical_fetures].values
  data = X_train_numerical_features
  scaler = MinMaxScaler()
  scaler.fit(data)
  pickle.dump(scaler, open('normelizers_numeric.pkl', 'wb'))
  X_train_numerical = scaler.transform(X_train_numerical_features)
  X_test_numerical = scaler.transform(X_test_numerical_features)
  return X_train_numerical , X_test_numerical

X_train_numerical , X_test_numerical = normalize_feature(numerical_features)

X_train_numerical.shape

X_train.loan_amnt.value_counts()

from sklearn.preprocessing import OneHotEncoder
def vectorize_feature(features):
  for i in features:
    vectorizer = OneHotEncoder(handle_unknown='ignore')
    vectorizer.fit(X_train[i].values.reshape(-1,1)) # fit has to happen only on train data
    globals()[f"vectorizer_{i}"] = vectorizer
    pickle.dump(globals()[f"vectorizer_{i}"], open(f'vectorizers/vectorizer_{i}.pkl', 'wb'))
    # we use the fitted OneHotEncoader to convert the text to vector
    X_train_vectors = vectorizer.transform(X_train[i].values.reshape(-1,1))
    X_test_vectors = vectorizer.transform(X_test[i].values.reshape(-1,1))

    globals()[f"X_train_{i}"] = X_train_vectors
    globals()[f"X_test_{i}"] = X_test_vectors
    print("After one hot encoading of "+i)
    print(X_train_vectors.shape, y_train.shape)
    print(X_test_vectors.shape, y_test.shape)
    print("="*100)

categorical_features_preprocessed = ['term','batch_enrolled','grade','sub_grade','emp_title','emp_length'
,'home_ownership','verification_status','purpose','title'
,'zip_code','addr_state','initial_list_status'
,'last_week_pay']
len(categorical_features_preprocessed)

vectorize_feature(categorical_features_preprocessed)

from scipy.sparse import hstack
X_train_final = hstack((X_train_numerical , X_train_term ,X_train_batch_enrolled,X_train_grade,X_train_sub_grade,X_train_emp_title,X_train_emp_length
,X_train_home_ownership,X_train_verification_status,X_train_purpose,X_train_title
,X_train_zip_code,X_train_addr_state,X_train_initial_list_status
,X_train_last_week_pay ))

pickle.dump(X_train_final, open('X_train_final.pkl', 'wb'))

X_test_final = hstack((X_test_numerical , X_test_term ,X_test_batch_enrolled,X_test_grade,X_test_sub_grade,X_test_emp_title,X_test_emp_length
,X_test_home_ownership,X_test_verification_status,X_test_purpose,X_test_title
,X_test_zip_code,X_test_addr_state,X_test_initial_list_status
,X_test_last_week_pay ))
pickle.dump(X_test_final, open('X_test_final.pkl', 'wb'))

pickle.dump(y_train, open('y_train.pkl', 'wb'))
pickle.dump(y_test, open('y_test.pkl', 'wb'))

pickle.dump(X_train_numerical, open('X_train_numerical.pkl', 'wb'))
pickle.dump(X_test_numerical, open('X_test_numerical.pkl', 'wb'))

X_train_final = pickle.load(open('X_train_final.pkl', 'rb'))
X_test_final = pickle.load(open('X_test_final.pkl', 'rb'))

X_train_csr = X_train_final.tocsr()
X_test_csr = X_test_final.tocsr()

y_train = pickle.load(open('y_train.pkl', 'rb'))
y_test = pickle.load(open('y_test.pkl', 'rb'))

X_test_final

"""#Apply Model

- first we apply logistic regression becase Liniar model give good results when dimentions are heigh
"""

def batch_predict(clf, data):
    """ this function predicts batch wise propablity 
        for given data over given model
    """

    y_data_pred = []
    tr_loop = data.shape[0] - data.shape[0]%1000
    for i in range(0, tr_loop, 1000):
        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])
    # we will be predicting for the last data points
    if data.shape[0]%1000 !=0:
        y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])
    
    return y_data_pred

from sklearn.model_selection import RandomizedSearchCV

"""##Logistic Regression"""

from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(random_state=15)
parameters = {'C':[1e-7,1e-6,1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]}
clf = RandomizedSearchCV(clf, parameters, cv=3 ,scoring='roc_auc' ,return_train_score=True,random_state=15)

clf.fit(X_train_csr,y_train)

results = pd.DataFrame.from_dict(clf.cv_results_)
print(results.columns)
results = results.sort_values(['param_C'])

train_auc= results['mean_train_score']
train_auc_std= results['std_train_score']
cv_auc = results['mean_test_score'] 
cv_auc_std= results['std_test_score']
K =  list(map(lambda x: np.log(x) , results['param_C']))
#K = results['param_C']
plt.plot(K, train_auc, label='Train AUC')

plt.plot(K, cv_auc, label='CV AUC')

plt.scatter(K, train_auc, label='Train AUC points')
plt.scatter(K, cv_auc, label='CV AUC points')


plt.legend()
plt.xlabel("log(C): hyperparameter")
plt.ylabel("AUC")
plt.title("Hyper parameter Vs AUC plot")
plt.grid()
plt.show()

results



#best  C is 1
from sklearn.metrics import roc_curve, auc

lr_model = LogisticRegression(C=1, random_state=15)
lr_model.fit(X_train_csr, y_train)
# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
# not the predicted outputs

y_train_pred = batch_predict(lr_model, X_train_csr)    
y_test_pred = batch_predict(lr_model, X_test_csr)

train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)
test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)

plt.plot(train_fpr, train_tpr, label="train AUC ="+str(auc(train_fpr, train_tpr)))
plt.plot(test_fpr, test_tpr, label="test AUC ="+str(auc(test_fpr, test_tpr)))
plt.legend()
plt.xlabel("FPR")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()
pickle.dump(lr_model, open('lr_model', 'wb'))

def find_best_threshold(threshould, fpr, tpr):
    t = threshould[np.argmax(tpr*(1-fpr))]
    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high
    print("the maximum value of tpr*(1-fpr)", max(tpr*(1-fpr)), "for threshold", np.round(t,3))
    return t

def predict_with_best_t(proba, threshould):
    predictions = []
    for i in proba:
        if i>=threshould:
            predictions.append(1)
        else:
            predictions.append(0)
    return predictions

import seaborn as sns
import matplotlib.pyplot as plt   
print("="*100)
from sklearn.metrics import confusion_matrix
best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)
print("Train confusion matrix")
train_predict = predict_with_best_t(y_train_pred, best_t)
cm_train = confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t))
#cm_train = confusion_matrix(y_train,train_predict)
#print(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)))
print("Test confusion matrix")
test_predict = predict_with_best_t(y_test_pred, best_t)
cm_test = confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t))
#cm_test = confusion_matrix(y_test, test_predict)
#print(confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)))

fig = plt.figure(figsize = (5,5)) # width x height
ax1 = fig.add_subplot(2,2,1) # row, column, position
ax2 = fig.add_subplot(2,2,3)
  
ax1.set_title('training set')
cm_train = pd.DataFrame(cm_train)
cm_train['Predicted'] = [0,1]
cm_train.columns.name = 'Actual'
cm_train.set_index('Predicted',inplace=True)
sns.heatmap(cm_train,ax=ax1,square=True, annot=True,fmt="d",cmap='Blues')

ax2.set_title('testing set')
cm_test = pd.DataFrame(cm_test)
cm_test['Predicted'] = [0,1]
cm_test.columns.name = 'Actual'
cm_test.set_index('Predicted',inplace=True)
sns.heatmap(cm_test,ax=ax2,square=True, annot=True,fmt="d",cmap='Blues')

"""## GBDT"""

import xgboost as xgb
from xgboost.sklearn import XGBClassifier

parameters = {
 'learning_rate':[0.05,0.1,0.2,0.3],
 'n_estimators':[10,50,120,200]
}
xgb = XGBClassifier( 
 max_depth=3,
 subsample=0.8,
 colsample_bytree=0.8)

random_search = RandomizedSearchCV(xgb,parameters,cv=3,return_train_score=True,random_state=15,scoring='roc_auc')
random_search.fit(X_train_csr,y_train)

results = pd.DataFrame.from_dict(random_search.cv_results_)

results

import plotly.offline as offline
import plotly.graph_objs as go
offline.init_notebook_mode()
import numpy as np
import matplotlib.pyplot as plt

"""### plese ensure you have installed plotly to see below 3d graph"""

x1,y1,z1 = results['param_n_estimators'],results['param_learning_rate'], results['mean_train_score']
x2,y2,z2 = results['param_n_estimators'],results['param_learning_rate'], results['mean_test_score']
trace1 = go.Scatter3d(x=x1,y=y1,z=z1, name = 'train')
trace2 = go.Scatter3d(x=x2,y=y2,z=z2, name = 'Cross validation')
data = [trace1, trace2]

layout = go.Layout(scene = dict(
        xaxis = dict(title='param_n_estimators'),
        yaxis = dict(title='param_learning_rate'),
        zaxis = dict(title='AUC'),))

fig = go.Figure(data=data, layout=layout)
offline.iplot(fig, filename='3d-scatter-colorscale')
fig.show(renderer="colab")
plt.show()

# {'n_estimators': 200, 'learning_rate': 0.3} these are our best params
from sklearn.metrics import roc_curve, auc
xgbClf = XGBClassifier( 
 learning_rate = 0.3,
 n_estimators=200,
 max_depth=3,
 subsample=0.8,
 colsample_bytree=0.8)
xgbClf.fit(X_train_csr, y_train)
# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
# not the predicted outputs

y_train_pred = batch_predict(xgbClf, X_train_csr)    
y_test_pred = batch_predict(xgbClf, X_test_csr)

train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)
test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)

plt.plot(train_fpr, train_tpr, label="train AUC ="+str(auc(train_fpr, train_tpr)))
plt.plot(test_fpr, test_tpr, label="test AUC ="+str(auc(test_fpr, test_tpr)))
plt.legend()
plt.xlabel("False positive rate")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()

from sklearn.externals import joblib

joblib_file = "xgb_model.joblib"
joblib.dump(xgbClf, joblib_file)

xgbClf = joblib.load('xgb_model.joblib')

print("="*100)
best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)
print("Train confusion matrix")
train_predict = predict_with_best_t(y_train_pred, best_t)
cm_train = confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t))
#cm_train = confusion_matrix(y_train,train_predict)
#print(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)))
print("Test confusion matrix")
test_predict = predict_with_best_t(y_test_pred, best_t)
cm_test = confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t))
#cm_test = confusion_matrix(y_test, test_predict)
#print(confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)))

fig = plt.figure(figsize = (5,5)) # width x height
ax1 = fig.add_subplot(2,2,1) # row, column, position
ax2 = fig.add_subplot(2,2,3)
  
ax1.set_title('training set')
cm_train = pd.DataFrame(cm_train)
cm_train['Predicted'] = [0,1]
cm_train.columns.name = 'Actual'
cm_train.set_index('Predicted',inplace=True)
sns.heatmap(cm_train,ax=ax1,square=True, annot=True,fmt="d",cmap='Blues')

ax2.set_title('testing set')
cm_test = pd.DataFrame(cm_test)
cm_test['Predicted'] = [0,1]
cm_test.columns.name = 'Actual'
cm_test.set_index('Predicted',inplace=True)
sns.heatmap(cm_test,ax=ax2,square=True, annot=True,fmt="d",cmap='Blues',)

"""## Random Forest"""

from sklearn.ensemble import RandomForestClassifier
parameters={'n_estimators':[10,30,50,80,100],'max_depth': [1, 5, 10, 50]}
clf_tree=RandomForestClassifier()
random_search = RandomizedSearchCV(clf_tree,parameters,cv=3,return_train_score=True,random_state=15,n_iter=16,scoring='roc_auc')
random_search.fit(X_train_csr,y_train)

results = pd.DataFrame.from_dict(random_search.cv_results_)
print(results.columns)
results = results.sort_values(['param_max_depth'])

results



x1,y1,z1 = results['param_n_estimators'],results['param_max_depth'], results['mean_train_score']
x2,y2,z2 = results['param_n_estimators'],results['param_max_depth'], results['mean_test_score']
trace1 = go.Scatter3d(x=x1,y=y1,z=z1, name = 'train')
trace2 = go.Scatter3d(x=x2,y=y2,z=z2, name = 'Cross validation')
data = [trace1, trace2]

layout = go.Layout(scene = dict(
        xaxis = dict(title='param_n_estimators'),
        yaxis = dict(title='param_max_depth'),
        zaxis = dict(title='AUC'),))

fig = go.Figure(data=data, layout=layout)
offline.iplot(fig, filename='3d-scatter-colorscale')
fig.show(renderer="colab")
plt.show()

#{'n_estimators': 100, 'max_depth': 50} are the best parameters
dtree = RandomForestClassifier(n_estimators=100,max_depth=50,random_state=15)
dtree.fit(X_train_csr, y_train)
# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
# not the predicted outputs

y_train_pred = batch_predict(dtree, X_train_csr)    
y_test_pred = batch_predict(dtree, X_test_csr)

train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)
test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)

plt.plot(train_fpr, train_tpr, label="train AUC ="+str(auc(train_fpr, train_tpr)))
plt.plot(test_fpr, test_tpr, label="test AUC ="+str(auc(test_fpr, test_tpr)))
plt.legend()
plt.xlabel("alpha: hyperparameter")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()

print("="*100)
from sklearn.metrics import confusion_matrix
best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)
print("Train confusion matrix w2v")
train_predict = predict_with_best_t(y_train_pred, best_t)
cm_train = confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t))
#cm_train = confusion_matrix(y_train,train_predict)
#print(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)))
print("Test confusion matrix w2v")
test_predict = predict_with_best_t(y_test_pred, best_t)
cm_test = confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t))
#cm_test = confusion_matrix(y_test, test_predict)
#print(confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)))

fig = plt.figure(figsize = (5,5)) # width x height
ax1 = fig.add_subplot(2,2,1) # row, column, position
ax2 = fig.add_subplot(2,2,3)
  
ax1.set_title('training set')
cm_train = pd.DataFrame(cm_train)
cm_train['Predicted'] = [0,1]
cm_train.columns.name = 'Actual'
cm_train.set_index('Predicted',inplace=True)
sns.heatmap(cm_train,ax=ax1,square=True, annot=True,fmt="d",cmap='Blues')

ax2.set_title('testing set')
cm_test = pd.DataFrame(cm_test)
cm_test['Predicted'] = [0,1]
cm_test.columns.name = 'Actual'
cm_test.set_index('Predicted',inplace=True)
sns.heatmap(cm_test,ax=ax2,square=True, annot=True,fmt="d",cmap='Blues')

joblib.dump(dtree, 'rdtree.joblib')

import tensorflow as tf
import random as rn
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import Callback
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow.keras.layers as layers
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Dense,Input,Dropout
from tensorflow.keras.models import Model

def aucroc(y_true, y_pred):
    m = tf.keras.metrics.AUC()
    m.update_state(y_true, y_pred)
    return m.result().numpy()

def chage_sparce_to_dence(sparce_mat):
  reaturn_mat = []
  for i in tqdm(range(0,sparce_mat.shape[0])):
    if reaturn_mat == []:
      reaturn_mat = sparce_mat[i,:].toarray()
      continue
    reaturn_mat = np.concatenate((reaturn_mat, sparce_mat[i,:].toarray() ), axis=0)
  return reaturn_mat

X_train_dence = pd.DataFrame.sparse.from_spmatrix(X_train_csr)
X_test_dence = pd.DataFrame.sparse.from_spmatrix(X_test_csr)

X_train_dence

"""#- Note i wrote a logic tried multiple times to run below nural network but because i was working on colab and its provide only 12gb ram it was got creashed during training so did not perform full training but i kept logic as it is """

X_input =  Input(shape=(X_train_dence.shape[1]), name= 'X_input', sparse=True)
Dence0 =  Dense(32,activation="relu",kernel_initializer='he_uniform')(X_input)

Dence1 =  Dense(32,activation="relu",kernel_initializer='he_uniform')(Dence0)
Dropout1 = Dropout(0.1)(Dence1)
Dence2 = Dense(64,activation="relu",kernel_initializer='he_uniform')(Dropout1)
BnLayer =  BatchNormalization()(Dence2)
Dropout2 = Dropout(0.1)(BnLayer)
Dence3 = Dense(64,activation="relu",kernel_initializer='he_uniform')(Dropout2)

output = Dense(1,activation="sigmoid",kernel_initializer='he_uniform')(Dence3)
model = Model(inputs=X_input, outputs=output)

earlystop = EarlyStopping(monitor='aucroc', min_delta=0.0000001, patience=4, verbose=2,mode='auto')

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999)

model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy',aucroc],run_eagerly=True)
save_model = model.fit( x=X_train_dence,y=y_train,batch_size=100,epochs=25,validation_data=(X_test_dence,y_test),callbacks=[earlystop])

"""#Load and Predict Funtions"""

train_data_preprocessed = pd.read_pickle("train_data_preprocessed.pkl")
train_data_preprocessed.columns

"""# **Result summary in tabular form**

<table class="styled-table" >
    <thead>
        <tr>
            <th>Model</th>
            <th>Hyperparameter</th>
            <th>Validation Auc</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Logistic regression classifier</td>
            <td>C = 1</td>
            <td>0.879</td>
        </tr>
        <tr class="active-row">
            <td>GBDT classifier</td>
            <td>n_estimators = 200 , learning_rate = 0.3</td>
            <td>0.9398</td>
        </tr>
        <tr class="active-row">
            <td>Random Forest</td>
            <td>n_estimators = 100 , max_depth = 50</td>
            <td>0.839</td>
        </tr>
        <!-- and so on... -->
    </tbody>
</table>

### so gbdt shows best auc score of 0.9398  with least overfitting so we will choose gbdt model to predict our submissions
"""

def text_operations(features):
  for i in features:
    train_data_preprocessed[i] = train_data_preprocessed[i].map(lambda x: x.lower().replace(' ','_'))

text_operations(['term','batch_enrolled','grade','sub_grade','emp_title','emp_length'
,'home_ownership','verification_status','purpose','title'
,'zip_code','addr_state','initial_list_status'
,'last_week_pay'])

def predict_paybility(row):
  annual_inc = row.annual_inc 
  if row.annual_inc == 0:
    #small value to ignore log(0) error
    annual_inc = 0.0001
  return (math.log(row.loan_amnt + (float(row.term.split('_')[0]) * row.int_rate)) + 0.00001 )/math.log(annual_inc)

import math
from scipy.sparse import hstack
from sklearn.externals import joblib
def load_predict():
  test_data = pd.read_csv('test_indessa.csv')
  member_id = test_data.member_id
  
  # select selected features which is in train_preprocessing
  test_data = test_data[['loan_amnt', 'term', 'batch_enrolled', 'int_rate', 'grade', 'sub_grade',
        'emp_title', 'emp_length', 'home_ownership', 'annual_inc',
        'verification_status', 'purpose', 'title', 'zip_code', 'addr_state',
        'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec',
        'revol_bal', 'revol_util', 'total_acc', 'initial_list_status',
        'total_rec_int', 'total_rec_late_fee', 'recoveries',
        'collection_recovery_fee', 'collections_12_mths_ex_med',
        'last_week_pay', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal',
        'total_rev_hi_lim']]
  
  #replace nan and black spaces in batch enrolled
  test_data['batch_enrolled'] = test_data['batch_enrolled'].fillna('unknown')
  test_data['batch_enrolled'] = test_data['batch_enrolled'].map(lambda x: 'blank' if x == ' ' else x)
  
  # fill nan values
  median_value_features = pickle.load(open('save_feature_median_value.pickle', 'rb'))
  max_value_features = pickle.load(open('save_feature_max_value.pickle', 'rb'))
  for i in median_value_features.keys():
    test_data[i] = test_data[i].fillna(median_value_features[i])
  for j in max_value_features.keys():
    test_data[j] = test_data[j].fillna(max_value_features[j])

  #preprocess categorica data
  cat_features = ['term','batch_enrolled','grade','sub_grade','emp_title','emp_length'
  ,'home_ownership','verification_status','purpose','title'
  ,'zip_code','addr_state','initial_list_status'
  ,'last_week_pay']
  for i in cat_features:
      test_data[i] = test_data[i].map(lambda x: x.lower().replace(' ','_'))

  #create new feature
  test_data['paybility'] =  test_data.apply(lambda row: predict_paybility(row), axis=1)

  #normalize numerical features
  numerical_features = ['loan_amnt', 'int_rate',
        'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc',
        'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'total_rec_int',
        'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',
        'collections_12_mths_ex_med', 'acc_now_delinq', 'tot_coll_amt',
        'tot_cur_bal', 'total_rev_hi_lim','paybility']
  normelizer_scaler = pickle.load(open('normelizers_numeric.pkl', 'rb'))
  test_data_numerical_features = test_data[numerical_features].values  
  test_data_numerical = normelizer_scaler.transform(test_data_numerical_features)

  #one hot encode categorical features
  categorical_features = ['term','batch_enrolled','grade','sub_grade','emp_title','emp_length'
  ,'home_ownership','verification_status','purpose','title'
  ,'zip_code','addr_state','initial_list_status'
  ,'last_week_pay']
  for i in categorical_features:
    vector = pickle.load(open(f'vectorizers/vectorizer_{i}.pkl', 'rb'))
    test_feature = vector.transform(test_data[i].values.reshape(-1,1))
    globals()[f"test_feature_{i}"] = test_feature

  test_data_final = hstack((test_data_numerical , test_feature_term ,test_feature_batch_enrolled,test_feature_grade,test_feature_sub_grade,test_feature_emp_title,test_feature_emp_length
  ,test_feature_home_ownership,test_feature_verification_status,test_feature_purpose,test_feature_title
  ,test_feature_zip_code,test_feature_addr_state,test_feature_initial_list_status
  ,test_feature_last_week_pay )).tocsr()

  #predict probablities
  xgbClf = joblib.load('xgb_model.joblib') 
  y_probabilities = batch_predict(xgbClf , test_data_final )
  test_submission_df = pd.DataFrame({'member_id':member_id, 'loan_status':y_probabilities})
  test_submission_df.to_csv('test_submission.csv')
  test_submission_df.head()

load_predict()

